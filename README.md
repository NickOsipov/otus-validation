# OTUS. Авто-валидация моделей

Проект демонстрирует полный MLOps pipeline для задачи классификации вредоносных URL с использованием статистически обоснованного A/B тестирования.

## Основные возможности

- **Обучение базовых моделей** с автоматической регистрацией в MLflow
- **A/B тестирование** с bootstrap-анализом и статистическими тестами
- **Автоматическое развертывание** моделей на основе статистической значимости
- **Оптимизация гиперпараметров** с помощью RandomizedSearchCV
- **Интеграция с MLflow** для версионирования и управления моделями

## Структура проекта

```
├── README.md              # Документация проекта
├── demo.sh                # Демонстрационный скрипт
├── docker-compose.yml     # Docker композиция
├── pyproject.toml         # Конфигурация проекта и зависимости
├── requirements.txt       # Список зависимостей
├── uv.lock                # Файл блокировки зависимостей uv
├── wait-for-it.sh         # Скрипт ожидания сервисов
├── infra/                 # Инфраструктурные файлы
│   └── mlflow/           # Конфигурация MLflow
│       ├── Dockerfile
│       └── requirements.txt
├── notebooks/             # Jupyter ноутбуки
│   └── work.ipynb        # Рабочий ноутбук
└── src/                   # Исходный код
    ├── __init__.py       # Инициализация пакета
    ├── common.py         # Общие функции для работы с данными и MLflow
    ├── train.py          # Обучение базовой модели
    ├── ab_test.py        # A/B тестирование и оптимизация
    └── examples.py       # Примеры использования
```

## Быстрый старт

### 1. Установка зависимостей

```bash
uv sync
```

### 2. Запуск MLflow сервера (опционально)

```bash
docker-compose up -d --build
```

### 3. Обучение базовой модели

```bash
python3 src/train.py --model-name url_classifier
```

### 4. A/B тестирование

```bash
python3 src/ab_test.py --model-name url_classifier --auto-deploy
```

## Подробное использование

### train.py - Обучение базовой модели

Обучает простую модель RandomForest и регистрирует её в MLflow как Production модель.

#### Параметры:
- `--model-name`: Имя модели в MLflow (по умолчанию: `url_classifier`)
- `--n-estimators`: Количество деревьев (по умолчанию: 35)
- `--max-depth`: Максимальная глубина (по умолчанию: 9)
- `--sample-frac`: Доля данных для использования (по умолчанию: 0.1)
- `--no-prod`: Не регистрировать как Production модель

#### Примеры:

```bash
# Базовое обучение
python3 src/train.py

# Обучение с кастомными параметрами
python3 src/train.py --n-estimators 50 --max-depth 15 --sample-frac 0.2

# Обучение без регистрации в Production
python3 src/train.py --no-prod
```

### ab_test.py - A/B тестирование

Выполняет оптимизацию гиперпараметров, сравнивает новую модель с Production моделью с помощью статистических тестов.

#### Параметры:
- `--model-name`: Имя модели в MLflow
- `--n-iter`: Количество итераций поиска гиперпараметров (по умолчанию: 50)
- `--cv`: Количество фолдов кросс-валидации (по умолчанию: 5)
- `--bootstrap-iterations`: Количество итераций bootstrap (по умолчанию: 100)
- `--alpha`: Уровень значимости (по умолчанию: 0.01)
- `--auto-deploy`: Автоматически разворачивать лучшую модель

#### Примеры:

```bash
# Базовое A/B тестирование
python3 src/ab_test.py

# A/B тест с автоматическим развертыванием
python3 src/ab_test.py --auto-deploy

# A/B тест с расширенным поиском
python3 src/ab_test.py --n-iter 100 --bootstrap-iterations 200

# Более строгий статистический критерий
python3 src/ab_test.py --alpha 0.001
```

## Методология A/B тестирования

### 1. Bootstrap-анализ
- Выполняется 100 итераций bootstrap для оценки стабильности метрик
- Позволяет получить распределение метрик без предположений о нормальности

### 2. Статистическое сравнение
- **t-тест** для проверки значимости различий между моделями
- **Размер эффекта (Cohen's d)** для оценки практической значимости
- **Доверительные интервалы** для оценки неопределенности

### 3. Критерии развертывания
Новая модель разворачивается только если:
- F1-score статистически значимо лучше (p < α)
- Улучшение практически значимо (Cohen's d > 0.2)

### 4. Параметры по умолчанию
- **α = 0.01**: строгий критерий значимости
- **100 итераций bootstrap**: достаточно для стабильной оценки
- **5-fold CV**: баланс между качеством оценки и скоростью

## Метрики и логирование

### Метрики модели:
- **Precision**: точность предсказаний
- **Recall**: полнота предсказаний  
- **F1-score**: гармоническое среднее precision и recall
- **AUC**: площадь под ROC-кривой

### MLflow логирование:
- Все метрики автоматически логируются в MLflow
- Модели сохраняются с версионированием
- Автоматическое управление алиасами моделей

## Конфигурация MLflow

По умолчанию используется локальный MLflow сервер на `http://localhost:5000`.

Для изменения конфигурации, отредактируйте `setup_mlflow()` в `common.py`:

```python
setup_mlflow(
    tracking_uri="http://your-mlflow-server:5000",
    experiment_name="your_experiment_name"
)
```

## Docker интеграция

Проект интегрируется с существующей Docker инфраструктурой:

```bash
# Запуск MLflow через docker-compose
docker-compose up -d --build
```

## Расширение функциональности

### Добавление новых метрик:
1. Обновите `evaluate_model()` в `common.py`
2. Добавьте метрику в `bootstrap_metrics()`
3. Включите в `statistical_comparison()`

### Новые алгоритмы:
1. Создайте новую функцию создания pipeline в `common.py`
2. Обновите `optimize_hyperparameters()` в `ab_test.py`

### Интеграция с другими ML платформами:
1. Замените MLflow функции в `common.py`
2. Сохраните интерфейсы для совместимости

## Логи и отладка

Все скрипты выводят подробную информацию о процессе выполнения:
- Успешные операции
- Ошибки и предупреждения  
- Промежуточные результаты
- Финальные метрики и решения
